{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uOh4fQaX4uo8"
      },
      "source": [
        "# Step 1: Importing Libraries and Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## In this section, we import the necessary libraries and load the EEG data of four different patients (0013, 0025, 0053, and 0090).\n",
        "## The data is then stored in variables POS_x and NEG_x, where x represents the patient number. \n",
        "## The POS_x variable contains the IED-positive epochs, and the NEG_x variable contains the IED-negative epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "D8OGw_-0JN00",
        "outputId": "4da99924-d898-42e5-e5ea-db9079fee71a"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import scipy.io\n",
        "import mat73\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow import keras\n",
        "from sklearn.utils import shuffle\n",
        "import collections\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split \n",
        "import pickle\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.stats import kurtosis\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checks if GPUs are available\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Prints the TensorFlow version\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loads the EEG data of four different patients (0013, 0025, 0053, and 0090)\n",
        "\n",
        "path = '/export03/data/Yuxin/pt_data/' # Defines the paths to the data\n",
        "\n",
        "POS_0013 = mat73.loadmat(path + 'pt0013/POS.mat')['POS']\n",
        "NEG_0013 = mat73.loadmat(path + 'pt0013/NEG.mat')['NEG']\n",
        "\n",
        "POS_0025 = mat73.loadmat(path + 'pt0025/POS.mat')['POS']\n",
        "NEG_0025 = mat73.loadmat(path + 'pt0025/NEG.mat')['NEG']\n",
        "\n",
        "POS_0053 = mat73.loadmat(path + 'pt0053/POS.mat')['POS']\n",
        "NEG_0053 = mat73.loadmat(path + 'pt0053/NEG.mat')['NEG']\n",
        "\n",
        "POS_0090 = mat73.loadmat(path + 'pt0090/POS.mat')['POS']\n",
        "NEG_0090 = mat73.loadmat(path + 'pt0090/NEG.mat')['NEG']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "62BbOUpx49DY"
      },
      "source": [
        "# Step 2: EEG Data Preprocessing for Neural Network Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## In this section, we process the EEG data using *PCs ordering method* to prepare it for neural network training. \n",
        "## The data is transposed to ensure it has the correct shape for training, and then epoching is applied to split the data into 1-second epochs. \n",
        "## Next, the data is scaled to a range between 0 and 1 using the MinMaxScaler method. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## By Raymundo Cassani, used with permission from: https://github.com/MuSAELab/amplitude-modulation-analysis-module/blob/master/am_analysis/am_analysis.py\n",
        "\n",
        "def epoching(data, samples_epoch, samples_overlap = 0):\n",
        "    \"\"\"Divide an array in a colletion of smaller arrays\n",
        "    \n",
        "    Divides the `data` provided as [n_samples, n_channels] using the \n",
        "    `size_epoch` indicated (in samples) and the `overlap_epoch` between \n",
        "    consecutive epochs.\n",
        "   \n",
        "    Parameters\n",
        "    ----------\n",
        "    data : 2D array \n",
        "        with shape (n_samples, n_channels)\n",
        "    samples_epochs : \n",
        "        number of samples in smaller epochs\n",
        "        \n",
        "    samples_overlap : \n",
        "        number of samples for ovelap between epochs (Default 0)\n",
        "    Returns\n",
        "    -------\n",
        "    epochs : 3D array \n",
        "        with shape (samples_epoch, n_channels, n_epochs)\n",
        "    \n",
        "    remainder : 2D array \n",
        "        with the remaining data after last complete epoch\n",
        "    \n",
        "    ix_center : 1D array\n",
        "        indicates the index tha corresponds to the center of the nth epoch.\n",
        "    \"\"\" \n",
        "    # input 'data' as 2D matrix [samples, columns]\n",
        "    try:\n",
        "        data.shape[1]\n",
        "    except IndexError:\n",
        "        data = data[:, np.newaxis]\n",
        "    \n",
        "    # number of samples and number of channels\n",
        "    n_samples, n_channels = data.shape\n",
        "\n",
        "    # Size of half epoch\n",
        "    half_epoch = np.ceil(samples_epoch / 2 )\n",
        "\n",
        "    # Epoch shift   \n",
        "    samples_shift = samples_epoch - samples_overlap\n",
        "\n",
        "    # Number of epochs\n",
        "    n_epochs =  int(np.floor( (n_samples - samples_epoch) / float(samples_shift) ) + 1 )\n",
        "    if n_epochs == 0:\n",
        "        return np.array([]), data, np.array([])\n",
        "\n",
        "    #markers indicates where the epoch starts, and the epoch contains samples_epoch rows\n",
        "    markers = np.asarray(range(0,n_epochs)) * samples_shift\n",
        "    markers = markers.astype(int)\n",
        "\n",
        "    #Divide data in epochs\n",
        "    epochs = np.zeros((samples_epoch, n_channels, n_epochs))\n",
        "    ix_center = np.zeros((n_epochs,1))\n",
        "\n",
        "    for i_epoch in range(0,n_epochs):\n",
        "        epochs[:,:,i_epoch] = data[ markers[i_epoch] : markers[i_epoch] + samples_epoch ,:]\n",
        "        ix_center[i_epoch] = markers[i_epoch] -1 + half_epoch\n",
        "        \n",
        "    if ( (markers[-1] + samples_epoch) < n_samples): \n",
        "        remainder = data[markers[-1] + samples_epoch : n_samples, :]\n",
        "    else:\n",
        "        remainder = np.asarray([])\n",
        "    \n",
        "    return epochs, remainder, ix_center.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## This function scales each channel of the input data to a range of [0,1]\n",
        "## Input: 3D numpy array (channels, timepoints, trials)\n",
        "## Output: Scaled 3D numpy array with values in the range of [0,1]\n",
        "\n",
        "def scale_data(data):\n",
        "  counter = 0\n",
        "  return_copy = copy.deepcopy(data)\n",
        "  \n",
        "  # Loop over each channel\n",
        "  for item in data:\n",
        "    # Reshape to 2D array\n",
        "    one_column = np.reshape(item, (-1, 1))\n",
        "    \n",
        "    # Scale data to range [0,1]\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(one_column)\n",
        "    one_column = scaler.transform(one_column)\n",
        "    \n",
        "    # Reshape back to 3D array\n",
        "    transformed = np.reshape(one_column, (data.shape[1], data.shape[2]))\n",
        "    return_copy[counter] = transformed\n",
        "    counter=counter+1\n",
        "  \n",
        "  return return_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## This function processes raw EEG data with the *PCs ordering method* to prepare for neural network training.\n",
        "\n",
        "def process_raw_data(Data, EEG_start, EEG_end):\n",
        "\n",
        "    # The function takes raw data as input, and the start and end indices of EEG channels\n",
        "\n",
        "    # `EEG_start``: the starting index of EEG channels in the data\n",
        "    # `EEG_end``: the ending index of EEG channels in the data\n",
        "\n",
        "    # Make a deep copy of the input data to avoid overwriting the original data\n",
        "    deep_copy = copy.deepcopy(Data)\n",
        "\n",
        "    # Transpose the deep copied data\n",
        "    transposed = np.transpose(deep_copy)\n",
        "\n",
        "    # Slice the transposed data to extract EEG data\n",
        "    EEG = transposed[:,EEG_start-1:EEG_end]\n",
        "\n",
        "    # PCs ordering method ----------------------------------\n",
        "    # Perform PCA on the EEG data and select the first 20 principal components\n",
        "    n_components = 20\n",
        "    pca_EEG = PCA(n_components)\n",
        "    pca_EEG.fit(EEG)\n",
        "    EEG_transformed = pca_EEG.transform(EEG)\n",
        "    \n",
        "    # Sort the first 5 PCs by kurtosis\n",
        "    first_5_PCs = EEG_transformed[:, :5] # Extract the first 5 PCs from the transformed EEG data\n",
        "    kurtosis_values = kurtosis(first_5_PCs, axis=0) # Calculate kurtosis for each of the first 5 PCs\n",
        "    sorted_indices = np.argsort(kurtosis_values) # Sort the indices of the first 5 PCs based on their kurtosis values\n",
        "    sorted_first_5_PCs = first_5_PCs[:, sorted_indices] # Order the first 5 PCs based on the sorted indices\n",
        "\n",
        "    # Sort the first 2 PCs from the sorted_first_5_PCs by variance\n",
        "    first_2_sorted_PCs = sorted_first_5_PCs[:, :2] # Extract the first 2 PCs from the sorted_first_5_PCs\n",
        "    variances = np.var(first_2_sorted_PCs, axis=0) # Calculate variance for each of the first 2 sorted PCs\n",
        "    sorted_indices = np.argsort(variances)[::-1] # Sort the indices of the first 2 sorted PCs based on their variance values\n",
        "    sorted_first_2_PCs = first_2_sorted_PCs[:, sorted_indices] # Order the first 2 sorted PCs based on the sorted indices\n",
        "\n",
        "    # Replace the first 5 PCs with the sorted PCs in the transformed EEG data\n",
        "    EEG_transformed[:, :5] = sorted_first_5_PCs\n",
        "    EEG_transformed[:, :2] = sorted_first_2_PCs\n",
        "    # -------------------------------------------------------\n",
        "\n",
        "    # Epoching the data into 1 second intervals\n",
        "    EEG = epoching(EEG_transformed, 256, 0)[0]\n",
        "\n",
        "    # Transpose the data to match CNN input shape\n",
        "    EEG = EEG.transpose(2,0,1)\n",
        "\n",
        "    # Scale the data to range between 0 and 1 with MinMaxScaler\n",
        "    EEG = scale_data(EEG)\n",
        "\n",
        "    return EEG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For each of the four participants, we use the `process_raw_data` function to extract the EEG data from the appropriate channel range, \n",
        "# epoch the data into 1-second epochs, transpose the data, and scale it to the range between 0 and 1 using the scale_data function. \n",
        "# The resulting EEG data for each participant are stored in POS_EEG and NEG_EEG variables.\n",
        "\n",
        "POS_EEG_0013 = process_raw_data(POS_0013, 301, 321)\n",
        "NEG_EEG_0013 = process_raw_data(NEG_0013, 301, 321)\n",
        "\n",
        "POS_EEG_0025 = process_raw_data(POS_0025, 299, 321)\n",
        "NEG_EEG_0025 = process_raw_data(NEG_0025, 299, 321)\n",
        "\n",
        "POS_EEG_0053 = process_raw_data(POS_0053, 299, 327)\n",
        "NEG_EEG_0053 = process_raw_data(NEG_0053, 299, 327)\n",
        "\n",
        "POS_EEG_0090 = process_raw_data(POS_0090, 302, 357)\n",
        "NEG_EEG_0090 = process_raw_data(NEG_0090, 302, 357)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CSmg74v88Avx"
      },
      "source": [
        "# Step 3: Input Data Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here, we plot the averaged EEG IEDs (interictal epileptiform discharges) for patient 0013. \n",
        "# The first plot shows the averaged EEG IEDs for positive epochs (EEG IED_positive), \n",
        "# and the second plot shows the averaged EEG IEDs for negative epochs (EEG IED_negative). \n",
        "# The same plots are generated for the other three patients.\n",
        "\n",
        "fig_0013, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5), dpi=150)\n",
        "\n",
        "ax1.plot(np.mean(POS_EEG_0013, axis=0))\n",
        "ax1.invert_yaxis()\n",
        "ax1.set_title('Averaged EEG_IED_pos Epochs')\n",
        "ax1.set_xlabel('Sample number')\n",
        "ax1.set_ylabel('Amplitude')\n",
        "\n",
        "ax2.plot(np.mean(NEG_EEG_0013, axis=0))\n",
        "ax2.invert_yaxis()\n",
        "ax2.set_title('Averaged EEG_IED_neg Epochs')\n",
        "ax2.set_xlabel('Sample number')\n",
        "ax2.set_ylabel('Amplitude')\n",
        "\n",
        "fig_0013.suptitle('Patient 0013', fontsize=22)\n",
        "fig_0013.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig_0025, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5), dpi=150)\n",
        "\n",
        "ax1.plot(np.mean(POS_EEG_0025, axis=0))\n",
        "ax1.invert_yaxis()\n",
        "ax1.set_title('Averaged EEG_IED_pos Epochs')\n",
        "ax1.set_xlabel('Sample number')\n",
        "ax1.set_ylabel('Amplitude')\n",
        "\n",
        "ax2.plot(np.mean(NEG_EEG_0025, axis=0))\n",
        "ax2.invert_yaxis()\n",
        "ax2.set_title('Averaged EEG_IED_neg Epochs')\n",
        "ax2.set_xlabel('Sample number')\n",
        "ax2.set_ylabel('Amplitude')\n",
        "\n",
        "fig_0025.suptitle('Patient 0025', fontsize=22)\n",
        "fig_0025.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig_0053, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5), dpi=150)\n",
        "\n",
        "ax1.plot(np.mean(POS_EEG_0053, axis=0))\n",
        "ax1.invert_yaxis()\n",
        "ax1.set_title('Averaged EEG_IED_pos Epochs')\n",
        "ax1.set_xlabel('Sample number')\n",
        "ax1.set_ylabel('Amplitude')\n",
        "\n",
        "ax2.plot(np.mean(NEG_EEG_0053, axis=0))\n",
        "ax2.invert_yaxis()\n",
        "ax2.set_title('Averaged EEG_IED_neg Epochs')\n",
        "ax2.set_xlabel('Sample number')\n",
        "ax2.set_ylabel('Amplitude')\n",
        "\n",
        "fig_0053.suptitle('Patient 0053', fontsize=22)\n",
        "fig_0053.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig_0090, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5), dpi=150)\n",
        "\n",
        "ax1.plot(np.mean(POS_EEG_0090, axis=0))\n",
        "ax1.invert_yaxis()\n",
        "ax1.set_title('Averaged EEG_IED_pos Epochs')\n",
        "ax1.set_xlabel('Sample number')\n",
        "ax1.set_ylabel('Amplitude')\n",
        "\n",
        "ax2.plot(np.mean(NEG_EEG_0090, axis=0))\n",
        "ax2.invert_yaxis()\n",
        "ax2.set_title('Averaged EEG_IED_neg Epochs')\n",
        "ax2.set_xlabel('Sample number')\n",
        "ax2.set_ylabel('Amplitude')\n",
        "\n",
        "fig_0090.suptitle('Patient 0090', fontsize=22)\n",
        "fig_0090.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "44wqaSoxL02e"
      },
      "source": [
        "# Step 4: Prepare the EEG data for training and testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai7G7136KLhq",
        "outputId": "b1565e0e-42be-4c67-87a7-5014cb83228c"
      },
      "outputs": [],
      "source": [
        "## Concatenate the EEG data from patients 0013, 0025, 0053, and 0090 for training and testing\n",
        "## Note: Training and testing data do not overlap\n",
        "\n",
        "# Combine positive (IED) EEG data from all patients\n",
        "POS_EEG = np.concatenate((POS_EEG_0013, POS_EEG_0025, POS_EEG_0053, POS_EEG_0090))\n",
        "\n",
        "# Combine negative (non-IED) EEG data from all patients\n",
        "NEG_EEG = np.concatenate((NEG_EEG_0013, NEG_EEG_0025, NEG_EEG_0053, NEG_EEG_0090))\n",
        "\n",
        "print('===== Checking concatenated EEG data =====')\n",
        "\n",
        "EEG_all_X = np.concatenate((POS_EEG, NEG_EEG))\n",
        "print(EEG_all_X.shape)\n",
        "EEG_y_1s = np.ones(POS_EEG.shape[0]).astype(int)\n",
        "EEG_y_0s = np.zeros(NEG_EEG.shape[0]).astype(int)\n",
        "EEG_all_y = np.concatenate((EEG_y_1s, EEG_y_0s))\n",
        "print(EEG_all_y.shape)\n",
        "\n",
        "print('===== Checking Trainin / Testing / Validation data =====')\n",
        "\n",
        "# Create Training, Validation, and Test Sets from all of our available data\n",
        "# Use 70/15/15 split since dataset is relatively small\n",
        "EEG_X_train,EEG_X_val_test,EEG_y_train,EEG_y_val_test = train_test_split(EEG_all_X,EEG_all_y, test_size=0.3)\n",
        "EEG_X_val,EEG_X_test,EEG_y_val,EEG_y_test = train_test_split(EEG_X_val_test,EEG_y_val_test, test_size=0.5)\n",
        "print(EEG_X_train.shape)\n",
        "print(EEG_y_train.shape)\n",
        "print(EEG_X_val.shape)\n",
        "print(EEG_y_val.shape)\n",
        "print(EEG_X_test.shape)\n",
        "print(EEG_y_test.shape)\n",
        "\n",
        "# Make sure EEG data is shuffled proprely\n",
        "print(EEG_y_test[0:10])\n",
        "print(EEG_y_val[0:10])\n",
        "print(EEG_y_train[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Use EEG data from patients 0013, 0025, and 0053 for trainning and 00090 for testing\n",
        "\n",
        "# Combine positive (IED) EEG data from patients 0013, 0025, and 0053 for training\n",
        "POS_EEG_train = np.concatenate((POS_EEG_0013, POS_EEG_0025, POS_EEG_0053))\n",
        "\n",
        "# Combine negative (non-IED) EEG data from patients 0013, 0025, and 0053 for training\n",
        "NEG_EEG_train = np.concatenate((NEG_EEG_0013, NEG_EEG_0025, NEG_EEG_0053))\n",
        "\n",
        "# Use positive (IED) EEG data from patient 0090 for testing\n",
        "POS_EEG_test = POS_EEG_0090\n",
        "\n",
        "# Use negative (non-IED) EEG data from patient 0090 for testing\n",
        "NEG_EEG_test = NEG_EEG_0090\n",
        "\n",
        "print('===== Checking concatenated EEG data =====')\n",
        "\n",
        "EEG_all_X = np.concatenate((POS_EEG, NEG_EEG))\n",
        "print(EEG_all_X.shape)\n",
        "EEG_y_1s = np.ones(POS_EEG.shape[0]).astype(int)\n",
        "EEG_y_0s = np.zeros(NEG_EEG.shape[0]).astype(int)\n",
        "EEG_all_y = np.concatenate((EEG_y_1s, EEG_y_0s))\n",
        "print(EEG_all_y.shape)\n",
        "\n",
        "print('===== Checking Trainin / Testing / Validation data =====')\n",
        "\n",
        "# Create Training, Validation, and Test Sets from all of our available data\n",
        "# Use 70/15/15 split since dataset is relatively small\n",
        "EEG_X_train,EEG_X_val_test,EEG_y_train,EEG_y_val_test = train_test_split(EEG_all_X,EEG_all_y, test_size=0.3)\n",
        "EEG_X_val,EEG_X_test,EEG_y_val,EEG_y_test = train_test_split(EEG_X_val_test,EEG_y_val_test, test_size=0.5)\n",
        "print(EEG_X_train.shape)\n",
        "print(EEG_y_train.shape)\n",
        "print(EEG_X_val.shape)\n",
        "print(EEG_y_val.shape)\n",
        "print(EEG_X_test.shape)\n",
        "print(EEG_y_test.shape)\n",
        "\n",
        "# Make sure EEG data is shuffled proprely\n",
        "print(EEG_y_test[0:10])\n",
        "print(EEG_y_val[0:10])\n",
        "print(EEG_y_train[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "SRI-cLmwNLsg"
      },
      "outputs": [],
      "source": [
        "# Dump EEG train, test, and validation dataset as pickle file\n",
        "\n",
        "pickle.dump(EEG_X_train, open(path+\"EEG_X_train.pickle\",'wb'))\n",
        "pickle.dump(EEG_X_val, open(path+\"EEG_X_val.pickle\",'wb'))\n",
        "pickle.dump(EEG_X_test, open(path+\"EEG_X_test.pickle\",'wb'))\n",
        "pickle.dump(EEG_y_train, open(path+\"EEG_y_train.pickle\",'wb'))\n",
        "pickle.dump(EEG_y_val, open(path+\"EEG_y_val.pickle\",'wb'))\n",
        "pickle.dump(EEG_y_test, open(path+\"EEG_y_test.pickle\",'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_i4aBuYfxgjw"
      },
      "source": [
        "# Step 5: Hyperparameter Grid Search for EEG CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load EEG data for grid search\n",
        "EEG_X_grid_train = pickle.load(open(path+\"pickles/EEG_X_grid_train.pickle\",'rb'))\n",
        "EEG_y_grid_train = pickle.load(open(path+\"pickles/EEG_y_grid_train.pickle\",'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the type and shape of EEG data for grid search\n",
        "print(type(EEG_X_grid_train))\n",
        "print(type(EEG_y_grid_train))\n",
        "print(EEG_X_grid_train.shape)\n",
        "print(EEG_y_grid_train.shape)\n",
        "\n",
        "# Make sure EEG data is shuffled proprely\n",
        "print(EEG_y_grid_train[0:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to return a 2D CNN using given hyperparameters during cross validation\n",
        "\n",
        "def get_clf(dropout, filters, fullyneurons):\n",
        "  \n",
        "  # Create Sequential model\n",
        "  model = tf.keras.Sequential() \n",
        "\n",
        "  # Add first convolutional layer with specified number of filters, kernel size (3, 3), and input shape based on epoch size\n",
        "  model.add(layers.Conv2D(filters, (3, 3), padding=\"same\", input_shape=(256, 20, 1))) \n",
        "  model.add(layers.BatchNormalization())  # Add batch normalization layer for faster and more stable training\n",
        "  model.add(layers.Activation(\"relu\"))  # Add ReLU activation function\n",
        "  model.add(layers.MaxPool2D(pool_size=(2, 2)))  # Add max pooling layer to reduce spatial dimensions\n",
        "  model.add(layers.Dropout(dropout))  # Add dropout layer for regularization\n",
        "\n",
        "  # Add second convolutional layer with double the number of filters and the same kernel size (3, 3)\n",
        "  model.add(layers.Conv2D(filters*2, (3, 3), padding=\"same\")) \n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation(\"relu\"))\n",
        "  model.add(layers.MaxPool2D(pool_size=(2,2))) \n",
        "  model.add(layers.Dropout(dropout))\n",
        "\n",
        "  # Add third convolutional layer with triple the number of filters and the same kernel size (3, 3)\n",
        "  model.add(layers.Conv2D(filters*3, (3, 3), padding=\"same\")) \n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation(\"relu\"))\n",
        "  model.add(layers.MaxPool2D(pool_size=(2,2))) \n",
        "  model.add(layers.Dropout(dropout))\n",
        "\n",
        "  # Flatten the output of the convolutional layers\n",
        "  model.add(layers.Flatten())\n",
        "\n",
        "  # Add a fully connected layer with the specified number of neurons\n",
        "  model.add(layers.Dense(fullyneurons))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation(\"relu\"))\n",
        "  model.add(layers.Dropout(dropout))\n",
        "\n",
        "  # Add the output layer with sigmoid activation function for binary classification: IED_positive or IED_negative\n",
        "  model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Define KerasClassifier for binary classification for EEG data\n",
        "\n",
        "# Reduce the learning rate if the loss does not improve for 2 epochs\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"loss\", factor=0.1, patience=2, min_lr=0.00001, model=\"auto\")\n",
        "\n",
        "# Stop training if the validation loss does not decrease for 10 epochs\n",
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "cb = [reduce_lr, early_stop] # Call back function\n",
        "\n",
        "clf = KerasClassifier(\n",
        "    model=get_clf,\n",
        "    loss=\"binary_crossentropy\", # Set loss fundtion to binary crossentropy\n",
        "    optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
        "    model__dropout=0.2,\n",
        "    model__filters = 8, \n",
        "    model__fullyneurons = 50,\n",
        "    fit__validation_split=0.2,\n",
        "    verbose=False,\n",
        "    epochs=100, # Maximum number of epochs\n",
        "    callbacks=cb,\n",
        "    metrics=[\"accuracy\", tf.keras.metrics.AUC(curve=\"ROC\", from_logits=True), tf.keras.metrics.AUC(curve=\"PR\", from_logits=True), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the scoring metrics to be used during grid search\n",
        "scoring_metrics = ['accuracy', 'roc_auc']\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "params = {\n",
        "    'model__dropout': [0.2, 0.4],  # Dropout rates to try\n",
        "    'model__filters': [8, 16, 32],  # Number of filters for the first convolutional layer\n",
        "    'model__fullyneurons': [50, 100, 500]  # Number of neurons in the fully connected layer\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object with the specified parameters, scoring metrics, and cross-validation settings\n",
        "gs = GridSearchCV(\n",
        "    clf,  # The classifier object to perform the grid search on\n",
        "    params,  # The hyperparameter search space\n",
        "    scoring=scoring_metrics,  # Scoring metrics to evaluate during the grid search\n",
        "    refit='accuracy',  # Refit the model with the best hyperparameters found for the accuracy metric\n",
        "    n_jobs=1,  # Number of CPU cores to use (1 for single core)\n",
        "    verbose=3,  # Verbosity level of the output messages\n",
        "    cv=3  # Number of cross-validation folds\n",
        ")\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "gs.fit(EEG_X_grid_train, EEG_y_grid_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the best score and the best set of hyperparameters found by the GridSearchCV function\n",
        "print(gs.best_score_, gs.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the grid search results of EEG data to CSV file\n",
        "gs_dict = gs.cv_results_\n",
        "del gs_dict['mean_fit_time']\n",
        "del gs_dict['std_fit_time']\n",
        "del gs_dict['mean_score_time']\n",
        "del gs_dict['std_score_time']\n",
        "del gs_dict['params']\n",
        "gs_df = pd.DataFrame.from_dict(gs_dict)\n",
        "gs_df.to_csv(path+\"eeg_gs_df_edited.csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "867Vs-2LJqQ8"
      },
      "source": [
        "# Step 6: Training and Testing EEG CNN Model Using Optimal Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtvG5SBgNwYa"
      },
      "outputs": [],
      "source": [
        "# Load EEG data for model training, validation, and testing\n",
        "EEG_X_train = pickle.load(open(path+\"EEG_X_train.pickle\",'rb'))\n",
        "EEG_X_val = pickle.load(open(path+\"EEG_X_val.pickle\",'rb'))\n",
        "EEG_X_test = pickle.load(open(path+\"EEG_X_test.pickle\",'rb'))\n",
        "EEG_y_train = pickle.load(open(path+\"EEG_y_train.pickle\",'rb'))\n",
        "EEG_y_val = pickle.load(open(path+\"EEG_y_val.pickle\",'rb'))\n",
        "EEG_y_test = pickle.load(open(path+\"EEG_y_test.pickle\",'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-7o0LfR_qAU"
      },
      "outputs": [],
      "source": [
        "## Create 2D CNN using optimal parameters\n",
        "\n",
        "model = tf.keras.Sequential() # Create Sequential model\n",
        "\n",
        "DROPOUT = 0.2\n",
        "FILTERS = 32\n",
        "FULLUNEURONS = 50\n",
        "\n",
        "# Add first convolutional layer with specified number of filters, kernel size (3, 3), and input shape based on epoch size\n",
        "model.add(layers.Conv2D(FILTERS, (3, 3), padding=\"same\", input_shape=(256, 20, 1)))\n",
        "model.add(layers.BatchNormalization())  # Add batch normalization layer for faster and more stable training\n",
        "model.add(layers.Activation(\"relu\"))  # Add ReLU activation function\n",
        "model.add(layers.MaxPool2D(pool_size=(2, 2)))  # Add max pooling layer to reduce spatial dimensions\n",
        "model.add(layers.Dropout(DROPOUT))  # Add dropout layer for regularization\n",
        "\n",
        "# Add second convolutional layer with double the number of filters and the same kernel size (3, 3)\n",
        "model.add(layers.Conv2D(FILTERS*2, (3, 3), padding=\"same\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "model.add(layers.Dropout(DROPOUT))\n",
        "\n",
        "# Add third convolutional layer with triple the number of filters and the same kernel size (3, 3)\n",
        "model.add(layers.Conv2D(FILTERS*3, (3, 3), padding=\"same\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
        "model.add(layers.Dropout(DROPOUT))\n",
        "\n",
        "# Flatten the output of the convolutional layers\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Add a fully connected layer with the specified number of neurons\n",
        "model.add(layers.Dense(FULLUNEURONS)) \n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Activation(\"relu\"))\n",
        "model.add(layers.Dropout(DROPOUT))\n",
        "\n",
        "# Add the output layer with sigmoid activation function for binary classification: IED_positive or IED_negative\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "# Set up Adam optimizer\n",
        "optimizer = Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
        "\n",
        "# Compile the model with the optimizer, loss function, and specified metrics\n",
        "model.compile(optimizer=optimizer, \n",
        "              loss=tf.keras.losses.BinaryCrossentropy(), \n",
        "              metrics=[\"accuracy\", \n",
        "                       tf.keras.metrics.AUC(curve=\"ROC\", from_logits=True), \n",
        "                       tf.keras.metrics.AUC(curve=\"PR\", from_logits=True), \n",
        "                       tf.keras.metrics.Precision(), \n",
        "                       tf.keras.metrics.Recall()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear the previous TensorFlow session\n",
        "keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the number of training epochs\n",
        "epochs = 20\n",
        "\n",
        "# Set up a checkpoint to save the model weights with the highest validation accuracy\n",
        "checkpoint = ModelCheckpoint(\"model_weights.h5\", \n",
        "                             monitor=\"val_accuracy\", \n",
        "                             save_weights_only=True, \n",
        "                             mode=\"max\", \n",
        "                             verbose=1)\n",
        "\n",
        "# Set up a learning rate scheduler to reduce the learning rate if the loss does not improve for 2 epochs\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"loss\", \n",
        "                              factor=0.1, \n",
        "                              patience=2, \n",
        "                              min_lr=0.00001, \n",
        "                              mode=\"auto\")\n",
        "\n",
        "# Create a list of callbacks to be used during training\n",
        "callbacks = [checkpoint, reduce_lr]\n",
        "\n",
        "# Train the model using the specified training and validation data, epochs, and callbacks\n",
        "history = model.fit(\n",
        "    x=EEG_X_train,\n",
        "    y=EEG_y_train,\n",
        "    validation_data=(EEG_X_val, EEG_y_val),\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThdeIbF99SFx",
        "outputId": "1ecf3948-eea7-4fdf-c2ef-e2163f813172"
      },
      "outputs": [],
      "source": [
        "# Evaluate the trained EEG IED detector\n",
        "results = model.evaluate(EEG_X_test, EEG_y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Generate the confusion matrix of the trained EEG IED detector on the test data\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(EEG_X_test)\n",
        "y_pred = np.round(y_pred).flatten()\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(EEG_y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(cm, cmap='Blues')\n",
        "\n",
        "# Add labels and colorbar\n",
        "ax.set_xticks(np.arange(2))\n",
        "ax.set_yticks(np.arange(2))\n",
        "ax.set_xticklabels(['Non-IED', 'IED'])\n",
        "ax.set_yticklabels(['Non-IED', 'IED'])\n",
        "plt.colorbar(im)\n",
        "\n",
        "# Add annotations to the plot\n",
        "thresh = cm.max() / 2\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(j, i, format(cm[i, j], 'd'),\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 7: Visualize the EGG CNN Model's Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_time_series(data, xlable, ylabel):\n",
        "  cur = data \n",
        "  plt.plot(cur)\n",
        "  plt.gca().invert_yaxis() #Invert to match the default reversed y-axis view in Brainstorm.\n",
        "  plt.xlabel(xlable)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the trained EEG CNN model to predict the test set\n",
        "prediction_test = model.predict(EEG_X_test)\n",
        "prediction_test = (prediction_test > 0.5).astype(np.float32)\n",
        "prediction_test = prediction_test.flatten()\n",
        "\n",
        "# Initialize empty lists for IED_positive and IED_negative indices\n",
        "positive_indices = []\n",
        "negative_indices = []\n",
        "\n",
        "# Iterate through the predicted labels and assign indices to positive and negative lists\n",
        "for i in range(0, len(prediction_test)):\n",
        "    if prediction_test[i] == 1:\n",
        "        positive_indices.append(i)\n",
        "    else:\n",
        "        negative_indices.append(i)\n",
        "\n",
        "# Calculate the average of the test set samples classified as positive (IEDs)\n",
        "predicted_test_positives = np.mean(EEG_X_test[positive_indices, :, :], axis=0)\n",
        "\n",
        "# Visualize the time series of the average predicted positives (IEDs)\n",
        "show_time_series(predicted_test_positives, \"Sample number\", \"Amplitude\")\n",
        "\n",
        "# Calculate the average of the test set samples classified as negative (non-IEDs)\n",
        "predicted_test_negatives = np.mean(EEG_X_test[negative_indices, :, :], axis=0)\n",
        "\n",
        "# Visualize the time series of the average predicted negatives (non-IEDs)\n",
        "show_time_series(predicted_test_negatives, \"Sample number\", \"Amplitude\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.15 ('tf')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "0a61f0f117e47c67f5cff4c930af262b470a3a5d056375380b51bd9f97195b3d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
